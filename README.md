ğŸ§  AssumptionsChecker Suite

Modular diagnostics for real-world ML workflows Built for deployment-aware pipelines, stakeholder clarity, and robust edge-case handling.

ğŸ” Overview

The AssumptionsChecker suite provides three modular tools â€” RegressionAssumptionsChecker and ClassificationAssumptionsChecker & DataIntegrityChecker â€” designed to surface hidden risks 
in machine learning models before they reach production. 

Whether you're validating a regression modelâ€™s residuals or stress-testing a classifierâ€™s decision boundaries, these tools help you clarify, not just compute.

âš™ï¸ Features

    âœ… Multicollinearity check via VIF with threshold flagging

    ğŸ“ˆ Residual diagnostics: normality, skewness, kurtosis, Q-Q plots

    ğŸ“Š Homoscedasticity tests: Breusch-Pagan, Goldfeld-Quandt

    ğŸ§  Influence analysis: Cookâ€™s distance, leverage scores

    ğŸ“‰ Classification diagnostics: class imbalance, confusion matrix, precision/recall drift

    ğŸ” Feature leakage detection via correlation and target leakage heuristics

    ğŸ§© Modular overlays: plug-and-play diagnostics for any ML pipeline

    ğŸ—£ï¸ Explanation-level control: toggle verbosity for technical vs stakeholder audiences

    ğŸ“¤ Export-ready reports: summary tables and visual diagnostics for review or presentation

ğŸ§  Why This Matters

Most ML workflows skip assumption testing â€” until something breaks. This suite makes it easy to surface hidden issues early, communicate risks clearly, and build trust with stakeholders. Itâ€™s built for deployment, not just notebooks.

ğŸ› ï¸ Roadmap

    [ ] SHAP integration for residual impact

    [ ] Streamlit dashboard for stakeholder review

    [ ] CI/CD hooks for automated diagnostics in MLOps pipelines

    [ ] Time-series support for autocorrelation and drift detection
